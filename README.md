台灣股市交易演算法系統（程式）現在能夠：

1.從 Yahoo Finance 獲取台灣股票數據
2.計算技術指標（MACD、ATR、EMA）
3.使用簡單的交易策略進行模擬交易
4.計算並顯示交易結果
5.生成資金曲線圖

概要說明：
這個程式是一個基於深度強化學習的台灣股市交易演算法系統。它利用深度Q學習(Deep Q-Learning)技術來訓練一個能夠自動進行股票交易決策的智能代理(agent)。系統通過分析歷史股價數據和技術指標，學習最佳的買入、賣出和持有策略，目標是在不同市場條件下最大化投資回報。這種方法結合了傳統的技術分析和現代的人工智能技術，為投資決策提供了一個數據驅動的框架。
程式與訓練過程的十大要點
1.	環境設計： 程式建立了一個模擬台灣股市的交易環境(TaiwanStockTradingEnv)，該環境遵循強化學習中的環境設計原則，包含狀態空間(觀察)、動作空間(買入、賣出、持有)和獎勵機制(基於交易回報)。
2.	數據獲取與處理： 系統使用yfinance庫從Yahoo Finance獲取台灣股票(如台積電2330.TW)和台灣加權指數(^TWII)的歷史數據，並進行預處理，包括數據清洗、標準化和技術指標的計算。
3.	特徵工程： 程式計算多種技術分析指標作為特徵，包括MACD(移動平均收斂散度)、EMA(指數移動平均)、ATR(真實波動幅度均值)等，這些指標幫助模型理解市場趨勢和波動性。
4.	代理設計： 交易代理基於深度Q網絡(DQN)實現，它通過神經網絡學習狀態-動作價值函數，從而決定在每個時間步應該採取的最佳行動(買入、賣出或持有)。
5.	訓練過程： 訓練過程採用經驗回放(Experience Replay)技術，代理與環境互動，收集經驗(狀態、動作、獎勵、下一狀態)，並使用這些經驗更新神經網絡參數，逐步改進交易策略。
6.	獎勵設計： 系統的獎勵函數主要基於交易回報率，同時考慮了交易成本和風險因素，目標是最大化整體投資組合的價值增長。
7.	評估機制： 程式包含完整的評估機制，用於測試訓練好的代理在未見過的市場數據上的表現，評估指標包括總回報率、夏普比率(Sharpe Ratio)和最大回撤等。
8.	視覺化展示： 系統提供了豐富的視覺化功能，包括資金曲線圖、股價走勢圖和交易信號標記，幫助使用者直觀地理解交易策略的表現和決策過程。
9.	結果分析： 程式在交易結束後會計算並顯示關鍵績效指標，如初始資金、最終資金、總回報率、交易次數和勝率等，並通過視窗界面呈現這些結果。
10.	實用功能： 系統還包含多種實用功能，如中文字體支持、交易結果保存、參數調整界面等，使其成為一個完整且用戶友好的交易演算法開發和測試平台。
這個系統不僅是一個交易演算法的實現，也是一個學習和研究強化學習在金融領域應用的優秀範例。通過調整不同的參數、特徵和獎勵函數，使用者可以探索各種交易策略，並根據台灣股市的特點進行優化。

台灣股市交易系統的買賣決策機制：
1. 買入與賣出的時間點
在這套交易系統中，買入與賣出的時間點並非基於傳統的固定規則或人為設定的閾值，而是由深度強化學習模型根據市場狀況動態決定的。具體來說：
•	買入時間點：當系統的深度Q網絡(DQN)評估當前市場狀態後，認為買入動作的預期長期回報高於持有或賣出時，系統會選擇買入。這通常發生在模型檢測到可能的上漲趨勢開始、技術指標顯示超賣狀態、或特定市場模式出現時。
•	賣出時間點：當深度Q網絡評估認為賣出動作的預期長期回報高於持有或買入時，系統會選擇賣出。這可能發生在模型檢測到可能的下跌趨勢開始、技術指標顯示超買狀態、或持倉獲利達到特定水平時。
重要的是，這些時間點不是預先編程的，而是模型通過與環境互動和學習過程中自動發現的。
2. 買入與賣出判斷的基礎
這套系統的買賣判斷基於以下幾個核心理論和計算基礎：
理論基礎：
1.	強化學習理論：系統基於馬爾可夫決策過程(MDP)框架，使用時間差分學習(TD-learning)方法來估計動作價值函數，並通過最大化長期累積獎勵來優化交易策略。
2.	深度學習理論：使用深度神經網絡來近似複雜的非線性價值函數，捕捉市場數據中的複雜模式和關係。
3.	技術分析理論：系統整合了多種技術分析指標，如MACD、EMA和ATR等，這些指標在傳統交易理論中被用於識別市場趨勢、動量和波動性。
計算基礎：
1.	狀態表示：系統將市場狀態表示為一個多維向量，包含股價數據、技術指標和市場指數等特徵。這些特徵經過標準化處理，以便神經網絡能夠有效學習。
2.	Q值計算：對於每個市場狀態，系統計算三個Q值，分別對應買入、賣出和持有動作。Q值代表從當前狀態開始，採取特定動作後的預期累積折現獎勵。
3.	動作選擇：系統選擇具有最高Q值的動作，即：
CopyInsert
動作 = argmax_a Q(狀態, a)
4.	獎勵信號：系統的獎勵主要基於交易後的資金變化，計算公式為：
CopyInsert
獎勵 = (當前資金 - 前一步資金) / 前一步資金
這個獎勵信號引導模型學習最大化投資回報率。
5.	神經網絡更新：系統使用均方誤差(MSE)損失函數和隨機梯度下降(SGD)優化器來更新神經網絡參數，逐步改進Q值估計的準確性。
總結來說，這套系統的買賣判斷不是基於單一理論或固定規則，而是一個綜合了強化學習、深度學習和技術分析的數據驅動方法。系統通過大量的交易經驗學習，逐步發現市場中的潛在模式和有效的交易策略，並根據這些學習結果做出買賣決策。這種方法的優勢在於能夠適應不同的市場條件，並隨著更多交易數據的積累而持續改進。
![trading_results](https://github.com/user-attachments/assets/abc166a1-b614-4693-9e96-d381f1ac99ca)
